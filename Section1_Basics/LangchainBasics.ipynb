{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24bfffb8",
   "metadata": {},
   "source": [
    "## 安装依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f87fd2",
   "metadata": {},
   "source": [
    "### 修改 pip 国内镜像源\n",
    "\n",
    "```bash\n",
    "pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703bc21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: langchain in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain) (0.3.24)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain) (2.11.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: langchain-ollama in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.47 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-ollama) (0.3.51)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (0.3.24)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (4.13.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.11.2)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: python-dotenv in d:\\myself\\grayjunzi\\learn-langchain\\learn-langchain_env\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-ollama\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33147aaa",
   "metadata": {},
   "source": [
    "# 与LLM交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d6ecb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\n\\n</think>\\n\\n您好！我是一个AI助手，由中国的深度求索（DeepSeek）公司独立开发，我清楚自己的身份与局限，会始终秉持专业和诚实的态度帮助用户。' additional_kwargs={} response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-04-09T13:37:32.9028602Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16770875800, 'load_duration': 7847106200, 'prompt_eval_count': 10, 'prompt_eval_duration': 1291000000, 'eval_count': 51, 'eval_duration': 7613000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-25f39018-5c24-4fe8-9c67-5f39e164428a-0' usage_metadata={'input_tokens': 10, 'output_tokens': 51, 'total_tokens': 61}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=250\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"你好，你今天怎么样？\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2728d",
   "metadata": {},
   "source": [
    "### Load Env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0705a51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load = load_dotenv('./../.env')\n",
    "\n",
    "print(os.getenv(\"LANGSMITH_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee08844",
   "metadata": {},
   "source": [
    "### 提示词与聊天模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d0dd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, so I'm trying to figure out why someone would want to run a large language model (LLM) on their local machine instead of using an online service. Hmm, let me think about this step by step.\\n\\nFirst, I know that LLMs are powerful tools that can generate text, answer questions, and perform various tasks. They're often accessed through platforms like ChatGPT or similar services. But running them locally means doing it on one's own computer, right? So what are the advantages of that?\\n\\nWell, maybe privacy is a concern. If I use an online service, my interactions with the model might be recorded or shared somewhere. Running it locally would keep all the data private. That makes sense because not everyone is comfortable with their inputs being stored elsewhere.\\n\\nAnother point could be control over the model. Maybe you can customize the LLM to suit your specific needs. Like, if I'm working on a project that requires a certain type of language model, having it on my machine allows me to tweak settings or add features without waiting for an online service to respond.\\n\\nSpeed might also play a role. If the internet is slow, waiting for the model to process each request could be frustrating. Running it locally might reduce latency since everything happens on the machine itself, though I'm not sure how that compares to optimized cloud processing.\\n\\nCost is another factor. Paying for access to online LLMs can add up, especially if you use them frequently. Owning a local model might save money in the long run, although there's the initial investment of hardware and time to train the model.\\n\\nI also wonder about flexibility. If I'm developing an application that relies on LLMs, having it locally allows for more seamless integration without relying on external APIs, which can be unstable or have usage limits.\\n\\nWait, but running a local LLM might require significant computational resources. High-end GPUs are needed for training and inference, which not everyone has access to. So maybe the advantage is more for those with the necessary hardware.\\n\\nI should also consider ease of use. If setting up a local LLM is complicated, it might not be worth it unless you have specific needs that require it. But for someone with the technical know-how, having control over their model could be beneficial.\\n\\nIn summary, the advantages likely include privacy, control over customization, potential speed benefits, cost savings over time, flexibility in application development, and avoiding external dependencies. However, these come with challenges like hardware requirements and setup complexity.\\n</think>\\n\\nThe advantages of running a large language model (LLM) on a local machine can be summarized as follows:\\n\\n1. **Privacy**: All interactions with the LLM remain entirely private, eliminating concerns about data storage and sharing with third parties.\\n\\n2. **Customization and Control**: Enables customization of the model to meet specific needs, allowing for adjustments and feature additions without waiting for external processing.\\n\\n3. **Reduced Latency**: Potentially faster processing as everything occurs locally, reducing reliance on internet connectivity which might be slow or unreliable.\\n\\n4. **Cost Efficiency**: Reduces long-term costs associated with paying for online access, though it involves initial investments in hardware and training.\\n\\n5. **Flexibility in Integration**: Facilitates seamless integration into applications without dependency on external APIs, enhancing stability and functionality.\\n\\n6. **Avoiding External Dependencies**: Reduces vulnerability to API limitations or outages by having full control over the model's operations.\\n\\nThese advantages are particularly beneficial for those with the necessary computational resources and technical expertise, offering enhanced privacy, customization, and reliability in their applications.\", additional_kwargs={}, response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-04-09T13:39:58.1402719Z', 'done': True, 'done_reason': 'stop', 'total_duration': 110825295500, 'load_duration': 49392800, 'prompt_eval_count': 15, 'prompt_eval_duration': 805000000, 'eval_count': 728, 'eval_duration': 109969000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-255a2976-8977-4f56-904a-fb83a4180e91-0', usage_metadata={'input_tokens': 15, 'output_tokens': 728, 'total_tokens': 743})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"What is the advantage of running the LLM in {env}\")\n",
    "\n",
    "prompt = prompt_template.invoke({\"env\":\"local machine\"})\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dce74",
   "metadata": {},
   "source": [
    "#### 聊天提示词模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3ef33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"<think>\\nOkay, so I'm trying to understand why someone might want to run AI models on their local machine instead of using cloud-based services. Let me start by thinking about what I know.\\n\\nI remember that when you use cloud-based platforms like AWS or Google Cloud, you can access really powerful AI models with just a few lines of code. That's super convenient because you don't have to manage anything locally. But maybe there are situations where running it on your own machine is better.\\n\\nOne reason could be data privacy. If the data I'm working with is sensitive, maybe I shouldn't send it off to a cloud service. It makes sense that some industries can't risk their data being stored elsewhere, so having control over where the data goes is important.\\n\\nAnother thought is about customization. Perhaps the AI model needs to be tweaked or modified in a specific way that isn't possible through an API. If I have access to the model's code, I could make changes without waiting for someone else to do it for me.\\n\\nCost might also play a role. If you're running this locally, maybe it's cheaper than paying for cloud services, especially if you don't need it 24/7 or at scale. It could be more cost-effective for small projects or one-time use.\\n\\nI've heard that some models can be slow when run on local machines, but maybe for certain tasks, the speed is manageable. Also, there's the aspect of control over performance. If you know your hardware setup, you might optimize it better than a cloud provider can.\\n\\nWait, what about open-source models? If I have access to the model's code, I could contribute or modify it, which isn't possible with black-box APIs. That could lead to innovation and faster development cycles.\\n\\nBut then there are challenges too. Maintaining hardware is a big deal—upgrading parts if needed. Also, some cloud services offer features like automatic scaling, which local setups might not have, so handling traffic spikes could be an issue.\\n\\nSo, putting it all together, the advantages seem to include data control, customization, cost savings, flexibility in performance, contributing to open-source, and innovation. But there are definitely trade-offs with maintenance and scalability that need to be considered.\\n\\nI wonder if there's more I haven't thought of, like legal compliance or specific hardware requirements for certain models. Maybe some AI models require specialized GPUs which can be expensive to run locally. On the other hand, cloud services might offer access to those GPUs without upfront costs.\\n\\nAnother point is the learning curve. If you're familiar with local computing environments, setting up a model there might be easier than dealing with cloud interfaces and APIs. It could save time in the long run.\\n\\nI'm also thinking about the future of AI development. If someone is experimenting or prototyping, having a local setup can speed up iterations without waiting for external services to process each step.\\n\\nSo, in summary, while cloud-based AI has its advantages like ease of use and scalability, running models locally offers control over data, customization, potential cost savings, flexibility, and the ability to contribute to open-source projects. However, it comes with responsibilities like hardware maintenance and scalability challenges.\\n</think>\\n\\nRunning AI models on a local machine offers several advantages, each addressing specific needs and scenarios:\\n\\n1. **Data Privacy and Control**: Local execution provides full control over data, essential for industries where sensitive information must be handled confidentially.\\n\\n2. **Customization and Innovation**: Access to model code allows for modifications, enabling innovation and tailored solutions that may not be feasible through APIs.\\n\\n3. **Cost Efficiency**: For small-scale or one-time projects, local running can be more economical than cloud services, especially when 24/7 availability isn't required.\\n\\n4. **Performance Optimization**: Control over hardware setup allows for better optimization of resources, potentially improving model performance and efficiency.\\n\\n5. **Open-Source Contribution**: Local access facilitates contributions to open-source projects, fostering collaboration and innovation in AI development.\\n\\n6. **Faster Development Cycles**: Local setups can accelerate experimentation and prototyping, reducing dependency on external services.\\n\\nHowever, it's important to consider challenges such as hardware maintenance, scalability issues, and the need for specialized hardware like GPUs, which may incur significant costs. Balancing these factors is crucial when deciding between local and cloud-based AI execution.\" additional_kwargs={} response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-04-09T13:42:45.4797798Z', 'done': True, 'done_reason': 'stop', 'total_duration': 134595134800, 'load_duration': 57471100, 'prompt_eval_count': 20, 'prompt_eval_duration': 1073000000, 'eval_count': 878, 'eval_duration': 132581000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-5ee892e4-8fb9-4905-bebf-e8aa2ecda47b-0' usage_metadata={'input_tokens': 20, 'output_tokens': 878, 'total_tokens': 898}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "\n",
    "systemMessage = SystemMessagePromptTemplate.from_template(\"You are an LLM expert\")\n",
    "humanMessage = HumanMessagePromptTemplate.from_template(\"What is the advantage of running AI Models in {env}\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    systemMessage,\n",
    "    humanMessage\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\"env\":\"local machine\"})\n",
    "\n",
    "content = llm.invoke(prompt)\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcb774",
   "metadata": {},
   "source": [
    "### 消息占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f3442d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "\n",
      "Okay\n",
      ",\n",
      " so\n",
      " I\n",
      "'m\n",
      " trying\n",
      " to\n",
      " figure\n",
      " out\n",
      " the\n",
      " advantages\n",
      " of\n",
      " running\n",
      " a\n",
      " Large\n",
      " Language\n",
      " Model\n",
      " (\n",
      "LL\n",
      "M\n",
      ")\n",
      " on\n",
      " a\n",
      " local\n",
      " machine\n",
      ".\n",
      " I\n",
      " know\n",
      " that\n",
      " usually\n",
      ",\n",
      " people\n",
      " use\n",
      " cloud\n",
      "-based\n",
      " models\n",
      " because\n",
      " they\n",
      "'re\n",
      " accessible\n",
      " from\n",
      " anywhere\n",
      " and\n",
      " don\n",
      "'t\n",
      " require\n",
      " a\n",
      " lot\n",
      " of\n",
      " hardware\n",
      ".\n",
      " But\n",
      " why\n",
      " would\n",
      " someone\n",
      " choose\n",
      " to\n",
      " run\n",
      " it\n",
      " locally\n",
      " instead\n",
      "?\n",
      "\n",
      "\n",
      "First\n",
      ",\n",
      " privacy\n",
      " might\n",
      " be\n",
      " a\n",
      " concern\n",
      ".\n",
      " If\n",
      " I\n",
      "'m\n",
      " working\n",
      " on\n",
      " something\n",
      " sensitive\n",
      " or\n",
      " confidential\n",
      ",\n",
      " having\n",
      " the\n",
      " model\n",
      " on\n",
      " my\n",
      " own\n",
      " machine\n",
      " means\n",
      " my\n",
      " data\n",
      " isn\n",
      "'t\n",
      " sent\n",
      " over\n",
      " the\n",
      " internet\n",
      ".\n",
      " That\n",
      " makes\n",
      " sense\n",
      " because\n",
      " sending\n",
      " data\n",
      " to\n",
      " the\n",
      " cloud\n",
      " could\n",
      " expose\n",
      " it\n",
      " to\n",
      " potential\n",
      " breaches\n",
      ".\n",
      "\n",
      "\n",
      "Another\n",
      " thought\n",
      " is\n",
      " about\n",
      " control\n",
      ".\n",
      " By\n",
      " running\n",
      " the\n",
      " model\n",
      " myself\n",
      ",\n",
      " I\n",
      " can\n",
      " decide\n",
      " how\n",
      " it\n",
      "'s\n",
      " used\n",
      " and\n",
      " ensure\n",
      " it\n",
      " compl\n",
      "ies\n",
      " with\n",
      " specific\n",
      " guidelines\n",
      ".\n",
      " Maybe\n",
      " for\n",
      " a\n",
      " business\n",
      ",\n",
      " having\n",
      " local\n",
      " access\n",
      " allows\n",
      " them\n",
      " to\n",
      " meet\n",
      " regulatory\n",
      " requirements\n",
      " without\n",
      " worrying\n",
      " about\n",
      " external\n",
      " policies\n",
      ".\n",
      "\n",
      "\n",
      "Custom\n",
      "ization\n",
      " could\n",
      " be\n",
      " another\n",
      " reason\n",
      ".\n",
      " If\n",
      " I\n",
      " need\n",
      " the\n",
      " model\n",
      " to\n",
      " perform\n",
      " specific\n",
      " tasks\n",
      " tailored\n",
      " to\n",
      " my\n",
      " needs\n",
      ",\n",
      " having\n",
      " it\n",
      " locally\n",
      " might\n",
      " allow\n",
      " me\n",
      " to\n",
      " tweak\n",
      " settings\n",
      " or\n",
      " integrate\n",
      " additional\n",
      " features\n",
      " more\n",
      " easily\n",
      " than\n",
      " relying\n",
      " on\n",
      " a\n",
      " pre\n",
      "-trained\n",
      " cloud\n",
      " model\n",
      ".\n",
      "\n",
      "\n",
      "Cost\n",
      " is\n",
      " probably\n",
      " a\n",
      " factor\n",
      " too\n",
      ".\n",
      " High\n",
      "-end\n",
      " models\n",
      " can\n",
      " be\n",
      " expensive\n",
      " to\n",
      " run\n",
      " in\n",
      " the\n",
      " cloud\n",
      ",\n",
      " especially\n",
      " with\n",
      " continuous\n",
      " usage\n",
      ".\n",
      " Ow\n",
      "ning\n",
      " the\n",
      " hardware\n",
      " could\n",
      " save\n",
      " money\n",
      " in\n",
      " the\n",
      " long\n",
      " run\n",
      ",\n",
      " although\n",
      " initial\n",
      " setup\n",
      " costs\n",
      " are\n",
      " high\n",
      ".\n",
      " Plus\n",
      ",\n",
      " if\n",
      " I\n",
      " have\n",
      " spare\n",
      " computing\n",
      " power\n",
      ",\n",
      " using\n",
      " it\n",
      " for\n",
      " an\n",
      " L\n",
      "LM\n",
      " might\n",
      " be\n",
      " more\n",
      " efficient\n",
      " than\n",
      " letting\n",
      " it\n",
      " idle\n",
      ".\n",
      "\n",
      "\n",
      "Ease\n",
      " of\n",
      " access\n",
      " is\n",
      " another\n",
      " consideration\n",
      ".\n",
      " If\n",
      " you\n",
      "'re\n",
      " in\n",
      " a\n",
      " remote\n",
      " area\n",
      " with\n",
      " poor\n",
      " internet\n",
      " connectivity\n",
      ",\n",
      " running\n",
      " a\n",
      " local\n",
      " model\n",
      " ensures\n",
      " you\n",
      " can\n",
      " still\n",
      " use\n",
      " it\n",
      " without\n",
      " relying\n",
      " on\n",
      " unstable\n",
      " connections\n",
      ".\n",
      " Also\n",
      ",\n",
      " for\n",
      " applications\n",
      " that\n",
      " require\n",
      " real\n",
      "-time\n",
      " processing\n",
      ",\n",
      " the\n",
      " latency\n",
      " from\n",
      " the\n",
      " cloud\n",
      " might\n",
      " be\n",
      " too\n",
      " much\n",
      ",\n",
      " and\n",
      " having\n",
      " it\n",
      " locally\n",
      " reduces\n",
      " that\n",
      " delay\n",
      ".\n",
      "\n",
      "\n",
      "I\n",
      " also\n",
      " think\n",
      " about\n",
      " experimentation\n",
      ".\n",
      " Local\n",
      " models\n",
      " might\n",
      " allow\n",
      " for\n",
      " easier\n",
      " fine\n",
      "-t\n",
      "uning\n",
      " and\n",
      " testing\n",
      " of\n",
      " different\n",
      " configurations\n",
      " without\n",
      " waiting\n",
      " for\n",
      " updates\n",
      " or\n",
      " changes\n",
      " to\n",
      " be\n",
      " implemented\n",
      " in\n",
      " the\n",
      " cloud\n",
      ".\n",
      " It\n",
      " could\n",
      " make\n",
      " iterative\n",
      " development\n",
      " faster\n",
      ".\n",
      "\n",
      "\n",
      "However\n",
      ",\n",
      " I\n",
      "'m\n",
      " not\n",
      " sure\n",
      " if\n",
      " these\n",
      " points\n",
      " cover\n",
      " everything\n",
      ".\n",
      " Maybe\n",
      " there\n",
      " are\n",
      " other\n",
      " benefits\n",
      " like\n",
      " performance\n",
      " improvements\n",
      " with\n",
      " lower\n",
      " latency\n",
      ",\n",
      " better\n",
      " resource\n",
      " utilization\n",
      " on\n",
      " local\n",
      " hardware\n",
      ",\n",
      " or\n",
      " the\n",
      " ability\n",
      " to\n",
      " use\n",
      " specialized\n",
      " hardware\n",
      " acceler\n",
      "ators\n",
      " which\n",
      " might\n",
      " not\n",
      " be\n",
      " available\n",
      " cloud\n",
      "-side\n",
      ".\n",
      "\n",
      "\n",
      "Wait\n",
      ",\n",
      " but\n",
      " running\n",
      " a\n",
      " local\n",
      " model\n",
      " requires\n",
      " significant\n",
      " computational\n",
      " resources\n",
      ".\n",
      " So\n",
      " it\n",
      "'s\n",
      " only\n",
      " feasible\n",
      " for\n",
      " those\n",
      " who\n",
      " have\n",
      " access\n",
      " to\n",
      " powerful\n",
      " machines\n",
      ".\n",
      " It\n",
      " might\n",
      " not\n",
      " be\n",
      " practical\n",
      " for\n",
      " everyone\n",
      ",\n",
      " but\n",
      " for\n",
      " certain\n",
      " use\n",
      " cases\n",
      ",\n",
      " like\n",
      " advanced\n",
      " research\n",
      " or\n",
      " high\n",
      "-st\n",
      "akes\n",
      " applications\n",
      ",\n",
      " it\n",
      " makes\n",
      " sense\n",
      ".\n",
      "\n",
      "\n",
      "I\n",
      "'m\n",
      " also\n",
      " wondering\n",
      " about\n",
      " scalability\n",
      " and\n",
      " flexibility\n",
      ".\n",
      " If\n",
      " you\n",
      " need\n",
      " the\n",
      " model\n",
      " to\n",
      " grow\n",
      " as\n",
      " your\n",
      " data\n",
      " grows\n",
      ",\n",
      " having\n",
      " it\n",
      " locally\n",
      " allows\n",
      " for\n",
      " easier\n",
      " integration\n",
      " with\n",
      " other\n",
      " tools\n",
      " and\n",
      " systems\n",
      " without\n",
      " relying\n",
      " on\n",
      " external\n",
      " APIs\n",
      ".\n",
      "\n",
      "\n",
      "In\n",
      " summary\n",
      ",\n",
      " running\n",
      " an\n",
      " L\n",
      "LM\n",
      " locally\n",
      " seems\n",
      " beneficial\n",
      " for\n",
      " privacy\n",
      ",\n",
      " control\n",
      ",\n",
      " customization\n",
      ",\n",
      " cost\n",
      " savings\n",
      " over\n",
      " time\n",
      ",\n",
      " better\n",
      " access\n",
      " in\n",
      " remote\n",
      " areas\n",
      ",\n",
      " real\n",
      "-time\n",
      " processing\n",
      " needs\n",
      ",\n",
      " ease\n",
      " of\n",
      " experimentation\n",
      ",\n",
      " possible\n",
      " hardware\n",
      " acceleration\n",
      ",\n",
      " and\n",
      " better\n",
      " resource\n",
      " utilization\n",
      ".\n",
      " But\n",
      " these\n",
      " come\n",
      " with\n",
      " the\n",
      " trade\n",
      "off\n",
      " of\n",
      " needing\n",
      " significant\n",
      " local\n",
      " computing\n",
      " power\n",
      ".\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "\n",
      "Running\n",
      " a\n",
      " Large\n",
      " Language\n",
      " Model\n",
      " (\n",
      "LL\n",
      "M\n",
      ")\n",
      " on\n",
      " a\n",
      " local\n",
      " machine\n",
      " offers\n",
      " several\n",
      " advantages\n",
      ",\n",
      " each\n",
      " addressing\n",
      " different\n",
      " needs\n",
      " and\n",
      " scenarios\n",
      ":\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " **\n",
      "Privacy\n",
      " and\n",
      " Data\n",
      " Control\n",
      "**:\n",
      " Loc\n",
      "ally\n",
      " running\n",
      " an\n",
      " L\n",
      "LM\n",
      " ensures\n",
      " that\n",
      " sensitive\n",
      " data\n",
      " remains\n",
      " on\n",
      " your\n",
      " machine\n",
      ",\n",
      " eliminating\n",
      " the\n",
      " risk\n",
      " of\n",
      " exposure\n",
      " through\n",
      " cloud\n",
      "-based\n",
      " services\n",
      ".\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " **\n",
      "Control\n",
      " and\n",
      " Compliance\n",
      "**:\n",
      " You\n",
      " maintain\n",
      " full\n",
      " control\n",
      " over\n",
      " the\n",
      " model\n",
      "'s\n",
      " usage\n",
      ",\n",
      " allowing\n",
      " compliance\n",
      " with\n",
      " specific\n",
      " regulatory\n",
      " requirements\n",
      " without\n",
      " dependency\n",
      " on\n",
      " external\n",
      " policies\n",
      ".\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " **\n",
      "Custom\n",
      "ization\n",
      " and\n",
      " Integration\n",
      "**:\n",
      " Enables\n",
      " tail\n",
      "oring\n",
      " the\n",
      " model\n",
      " to\n",
      " meet\n",
      " unique\n",
      " needs\n",
      ",\n",
      " with\n",
      " easier\n",
      " integration\n",
      " of\n",
      " additional\n",
      " features\n",
      " compared\n",
      " to\n",
      " pre\n",
      "-trained\n",
      " cloud\n",
      " models\n",
      ".\n",
      "\n",
      "\n",
      "4\n",
      ".\n",
      " **\n",
      "Cost\n",
      " Efficiency\n",
      "**:\n",
      " Red\n",
      "uces\n",
      " long\n",
      "-term\n",
      " costs\n",
      " by\n",
      " avoiding\n",
      " continuous\n",
      " cloud\n",
      " expenses\n",
      ",\n",
      " though\n",
      " initial\n",
      " setup\n",
      " requires\n",
      " significant\n",
      " investment\n",
      " in\n",
      " hardware\n",
      ".\n",
      "\n",
      "\n",
      "5\n",
      ".\n",
      " **\n",
      "Access\n",
      " and\n",
      " Connectivity\n",
      "**:\n",
      " Useful\n",
      " in\n",
      " areas\n",
      " with\n",
      " poor\n",
      " internet\n",
      " connectivity\n",
      " or\n",
      " for\n",
      " applications\n",
      " requiring\n",
      " real\n",
      "-time\n",
      " processing\n",
      " to\n",
      " minimize\n",
      " latency\n",
      " issues\n",
      ".\n",
      "\n",
      "\n",
      "6\n",
      ".\n",
      " **\n",
      "Experiment\n",
      "ation\n",
      " and\n",
      " Development\n",
      "**:\n",
      " Fac\n",
      "ilit\n",
      "ates\n",
      " easier\n",
      " fine\n",
      "-t\n",
      "uning\n",
      " and\n",
      " testing\n",
      " of\n",
      " configurations\n",
      ",\n",
      " aiding\n",
      " in\n",
      " iterative\n",
      " development\n",
      " without\n",
      " waiting\n",
      " for\n",
      " cloud\n",
      " updates\n",
      ".\n",
      "\n",
      "\n",
      "7\n",
      ".\n",
      " **\n",
      "Hardware\n",
      " Util\n",
      "ization\n",
      "**:\n",
      " Allows\n",
      " efficient\n",
      " use\n",
      " of\n",
      " local\n",
      " resources\n",
      ",\n",
      " potentially\n",
      " leveraging\n",
      " specialized\n",
      " hardware\n",
      " acceler\n",
      "ators\n",
      " not\n",
      " available\n",
      " in\n",
      " the\n",
      " cloud\n",
      ".\n",
      "\n",
      "\n",
      "8\n",
      ".\n",
      " **\n",
      "Sc\n",
      "al\n",
      "ability\n",
      " and\n",
      " Flex\n",
      "ibility\n",
      "**:\n",
      " Supports\n",
      " growth\n",
      " with\n",
      " data\n",
      " by\n",
      " integrating\n",
      " more\n",
      " easily\n",
      " with\n",
      " other\n",
      " tools\n",
      " and\n",
      " systems\n",
      ",\n",
      " enhancing\n",
      " overall\n",
      " flexibility\n",
      ".\n",
      "\n",
      "\n",
      "In\n",
      " conclusion\n",
      ",\n",
      " while\n",
      " running\n",
      " an\n",
      " L\n",
      "LM\n",
      " locally\n",
      " requires\n",
      " substantial\n",
      " computational\n",
      " resources\n",
      ",\n",
      " it\n",
      " is\n",
      " advantageous\n",
      " for\n",
      " privacy\n",
      ",\n",
      " control\n",
      ",\n",
      " customization\n",
      ",\n",
      " cost\n",
      " savings\n",
      ",\n",
      " access\n",
      ",\n",
      " real\n",
      "-time\n",
      " needs\n",
      ",\n",
      " experimentation\n",
      ",\n",
      " hardware\n",
      " utilization\n",
      ",\n",
      " and\n",
      " scalability\n",
      ",\n",
      " making\n",
      " it\n",
      " a\n",
      " suitable\n",
      " choice\n",
      " for\n",
      " specific\n",
      " use\n",
      " cases\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    MessagesPlaceholder(\"msg\")\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\"msg\":[HumanMessage(\"What is the advantage of running LLM in local machine\")]})\n",
    "\n",
    "for str in llm.stream(prompt):\n",
    "    print(str.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
