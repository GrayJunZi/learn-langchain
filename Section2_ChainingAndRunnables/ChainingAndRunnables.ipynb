{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659d260b",
   "metadata": {},
   "source": [
    "### 加载 .env 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b365ee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load = load_dotenv('./../.env')\n",
    "\n",
    "print(os.getenv(\"LANGSMITH_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b13c2",
   "metadata": {},
   "source": [
    "### 创建LLM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd20c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=250\n",
    ")\n",
    "\n",
    "llm2 = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c32f5",
   "metadata": {},
   "source": [
    "### 理解 Chanining & Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819c3af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, so I'm trying to figure out why someone might want to run AI models on their own machine instead of using cloud-based services. Let me think through this step by step.\\n\\nFirst, data privacy comes to mind. If I have sensitive information that I don't want to store somewhere else, running it locally makes sense. I can control where the data goes and ensure it's not stored online. That seems like a big advantage for certain industries like healthcare or finance where data security is crucial.\\n\\nThen there's latency and speed. Maybe if you're processing something in real-time, like trading on the stock market, having a model locally could reduce delays caused by sending data to the cloud and waiting for a response. That could be a big win for performance-critical applications.\\n\\nCost efficiency might also be a factor. If I'm running models frequently, paying for each run on the cloud can add up. Owning the hardware could save money in the long run, especially if the model is used a lot. Plus, once you've invested in the hardware, you don't have to pay for each computation.\\n\\nCustomizability is another point. If I need specific modifications or fine-tuning of a model, having it locally allows me to tweak it without worrying about internet connectivity or upload/download times. It gives more control over the model's performance.\\n\\nInteroperability could be important too. If you have existing systems on-premises, integrating a local AI model might be easier than setting up secure connections to cloud services. It streamlines the workflow and avoids potential issues with data movement.\\n\\nScalability is something to consider as well. Maybe my needs grow over time, and having hardware that I can scale by adding more GPUs or CPUs locally is better than relying on a third-party service which might have limitations or costs associated with scaling up.\\n\\nFlexibility in deployment also plays a role. If an AI solution is needed for a specific project without the overhead of managing cloud resources, having it run locally offers more flexibility and reduces the learning curve compared to dealing with cloud platforms.\\n\\nLastly, there's the aspect of innovation and experimentation. If I'm trying out different models or experimenting with new algorithms, running them locally allows me to iterate quickly without being constrained by internet access or API limits that might come with cloud services.\\n\\nSo putting it all together, the advantages seem to revolve around control over data, speed, cost savings, customization, integration with existing systems, scalability, flexibility, and the ability to innovate without external dependencies.\\n</think>\\n\\nRunning AI models locally offers several significant advantages:\\n\\n1. **Data Privacy and Control**: Local execution ensures that sensitive data remains on-premises, ideal for industries like healthcare and finance where security is paramount.\\n\\n2. **Latency and Speed**: Real-time processing benefits applications such as stock trading, where reduced delays can be critical to performance and decision-making.\\n\\n3. **Cost Efficiency**: Lower operational costs over time, especially with frequent model runs, as opposed to recurring cloud expenses.\\n\\n4. **Customizability**: Enables fine-tuning and modifications without reliance on external services, providing more control over model performance.\\n\\n5. **Interoperability**: Simplifies integration with existing on-premises systems, avoiding potential issues with data movement and connectivity.\\n\\n6. **Scalability**: Allows for easy expansion by adding hardware, addressing growing computational needs without third-party limitations.\\n\\n7. **Flexibility in Deployment**: Offers more control over AI solutions, reducing dependency on cloud resources and simplifying deployment processes.\\n\\n8. **Innovation and Experimentation**: Facilitates quick iteration and experimentation with models and algorithms, free from constraints imposed by external services.\\n\\nThese advantages collectively enhance control, performance, cost-effectiveness, and adaptability in AI applications.\", additional_kwargs={}, response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-04-12T09:21:12.4706176Z', 'done': True, 'done_reason': 'stop', 'total_duration': 101437108300, 'load_duration': 7280723800, 'prompt_eval_count': 20, 'prompt_eval_duration': 1246801300, 'eval_count': 758, 'eval_duration': 92901890600, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-2c0487c0-1516-44d5-ab6f-f15910ee968a-0', usage_metadata={'input_tokens': 20, 'output_tokens': 758, 'total_tokens': 778})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "chain.invoke({\"env\": \"local machine\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908f1ba",
   "metadata": {},
   "source": [
    "### 字符串解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6d531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out why someone might want to run AI models locally on their own machine instead of using the cloud. Hmm, let me think about this step by step.\n",
      "\n",
      "First, I know that when you run something in the cloud, like on AWS or Google Cloud, it's usually handled by powerful servers with lots of GPUs and TPUs. That means your model can access a lot of computational power, which is great for training large models quickly. But maybe there are situations where you don't want to send all that data off to a third party.\n",
      "\n",
      "Privacy and data security might be one reason. If the data is sensitive or contains personal information, sending it to the cloud could violate regulations like GDPR or HIPAA. So running it locally would keep the data on-premises, which is safer.\n",
      "\n",
      "Another thought is about latency and performance. If you're in a remote area with bad internet connectivity, uploading large datasets to the cloud might take forever. Processing them locally could save time and reduce delays in getting results back.\n",
      "\n",
      "Also, cost efficiency comes to mind. Training a model can be expensive if you're using cloud services because you pay per compute unit. If you already have the hardware on-site, you might save money instead of paying for cloud resources.\n",
      "\n",
      "I should also consider the technical skills needed. If someone is more comfortable with their own machine and has the necessary software installed, they might prefer that over dealing with cloud interfaces and setups. It could be easier to manage if you're familiar with the local environment.\n",
      "\n",
      "What about experimentation? Maybe researchers or developers want to tweak models frequently without waiting for cloud processing times. Running locally allows for rapid iteration and testing without relying on external systems.\n",
      "\n",
      "Wait, are there any downsides? Well, if the model is very large, running it locally might require a powerful machine, which could be expensive to set up and maintain. Also, managing the infrastructure could become a hassle if something breaks down.\n",
      "\n",
      "So putting this all together, the advantages of running AI models locally seem to include data privacy, reduced latency, cost savings, control over the environment, easier experimentation, and avoiding vendor lock-in. But it's also important to consider the potential challenges like hardware requirements and maintenance.\n",
      "</think>\n",
      "\n",
      "Running AI models locally offers several advantages, including:\n",
      "\n",
      "1. **Data Privacy and Security**: Keeping data on-premises can protect sensitive information, complying with regulations like GDPR or HIPAA.\n",
      "\n",
      "2. **Reduced Latency and Improved Performance**: Local processing can save time, especially in areas with poor internet connectivity, reducing delays in results.\n",
      "\n",
      "3. **Cost Efficiency**: Avoiding cloud costs by utilizing existing hardware can be more economical, especially for large models.\n",
      "\n",
      "4. **Control Over Environment**: Ensuring software and hardware configurations that suit specific needs without relying on external services.\n",
      "\n",
      "5. **Rapid Experimentation**: Allows for quick adjustments and testing without waiting for cloud processing times.\n",
      "\n",
      "6. **Avoiding Vendor Lock-In**: Maintaining control over data and infrastructure reduces dependency on third-party providers.\n",
      "\n",
      "However, considerations include the need for powerful local hardware and potential maintenance challenges. Balancing these factors can lead to efficient and secure AI model execution locally.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707cba3",
   "metadata": {},
   "source": [
    "### Chaining Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2bd963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out why someone might want to run AI models locally on their own machine instead of using the cloud. Hmm, let me think about this step by step.\n",
      "\n",
      "First, I know that when you run something in the cloud, like on AWS or Google Cloud, it's usually handled by powerful servers with lots of GPUs and TPUs. That means your model can access a lot of computational power, which is great for training large models quickly. But maybe there are situations where you don't want to send all that data off to a third party.\n",
      "\n",
      "Privacy and data security might be one reason. If the data is sensitive or contains personal information, sending it to the cloud could violate regulations like GDPR or HIPAA. So running it locally would keep the data on-premises, which is safer.\n",
      "\n",
      "Another thought is about latency and performance. If you're in a remote area with bad internet connectivity, uploading large datasets to the cloud might take forever. Processing them locally could save time and reduce delays in getting results back.\n",
      "\n",
      "Also, cost efficiency comes to mind. Training a model can be expensive if you're using cloud services because you pay per compute unit. If you already have the hardware on-site, you might save money instead of paying for cloud resources.\n",
      "\n",
      "I should also consider the technical skills needed. If someone is more comfortable with their own machine and has the necessary software installed, they might prefer that over dealing with cloud interfaces and setups. It could be easier to manage if you're familiar with the local environment.\n",
      "\n",
      "What about experimentation? Maybe researchers or developers want to tweak models frequently without waiting for cloud processing times. Running locally allows for rapid iteration and testing without relying on external systems.\n",
      "\n",
      "Wait, are there any downsides? Well, if the model is very large, running it locally might require a powerful machine, which could be expensive to set up and maintain. Also, managing the infrastructure could become a hassle if something breaks down.\n",
      "\n",
      "So putting this all together, the advantages of running AI models locally seem to include data privacy, reduced latency, cost savings, control over the environment, easier experimentation, and avoiding vendor lock-in. But it's also important to consider the potential challenges like hardware requirements and maintenance.\n",
      "</think>\n",
      "\n",
      "Running AI models locally offers several advantages, including:\n",
      "\n",
      "1. **Data Privacy and Security**: Keeping data on-premises can protect sensitive information, complying with regulations like GDPR or HIPAA.\n",
      "\n",
      "2. **Reduced Latency and Improved Performance**: Local processing can save time, especially in areas with poor internet connectivity, reducing delays in results.\n",
      "\n",
      "3. **Cost Efficiency**: Avoiding cloud costs by utilizing existing hardware can be more economical, especially for large models.\n",
      "\n",
      "4. **Control Over Environment**: Ensuring software and hardware configurations that suit specific needs without relying on external services.\n",
      "\n",
      "5. **Rapid Experimentation**: Allows for quick adjustments and testing without waiting for cloud processing times.\n",
      "\n",
      "6. **Avoiding Vendor Lock-In**: Maintaining control over data and infrastructure reduces dependency on third-party providers.\n",
      "\n",
      "However, considerations include the need for powerful local hardware and potential maintenance challenges. Balancing these factors can lead to efficient and secure AI model execution locally.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "detailedResponseChain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "headingInfoTemplate = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Analysis the resposne and get me just the heading from the {response}\n",
    "\n",
    "Response should be in bullet points \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chainWithHeading = { \"response\": detailedResponseChain } | headingInfoTemplate | llm | StrOutputParser()\n",
    "\n",
    "chainWithHeading.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39027db",
   "metadata": {},
   "source": [
    "### Running chains in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a09ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chain1': \"<think>\\nOkay, so I'm trying to figure out why someone would want to run AI models locally on their own machine instead of using a cloud service. I know that cloud services like AWS or Google Cloud offer powerful AI tools, but maybe there are situations where having it on your own machine is better.\\n\\nFirst, I think about data privacy and control. If you're working with sensitive data, maybe it's better not to send it off to a remote server. You can manage everything on your own machine without worrying about data breaches or who might access the data. That makes sense because not all industries can risk exposing their data like that.\\n\\nThen there's the issue of latency and speed. If you're processing something in real-time, like video analysis or something time-sensitive, having it on a local machine could be faster than waiting for a cloud service to respond. I remember reading that sometimes cloud services have delays, especially if they're really popular. So for something urgent, local processing might be more reliable.\\n\\nAnother point is the variety of models and flexibility. Maybe you need a specific model that's not available on the cloud platforms. Or perhaps you want to experiment with different configurations or fine-tune a model in a way that isn't possible through a standard API. Running it locally gives you more control over how you use the model.\\n\\nCost could also be a factor, especially for large-scale operations. If you're processing a huge amount of data and using cloud services charges based on usage, it might add up quickly. Running it locally might save money in the long run, even though setting up the hardware could be expensive initially.\\n\\nInteroperability is another thing. Some applications need AI models to work seamlessly with other tools or systems you already have installed. If those systems aren't designed to integrate with cloud services, having everything local would make integration easier and more efficient.\\n\\nCustomization and research might also play a role. For academic purposes or developing new models, having the code and data on your own machine allows for deeper experimentation and analysis. You can tweak models, run them multiple times, and see how small changes affect performance without waiting for cloud processing times.\\n\\nScalability is another consideration. If you're doing something that requires a lot of computational power, maybe you can build a more powerful local setup than what's available on the cloud, especially if you're dealing with very large datasets or complex computations that require more resources than typical cloud instances provide.\\n\\nI also think about accessibility and reliability. In remote locations with poor internet connectivity, accessing cloud services might be difficult. Having everything locally ensures that your AI models can run without relying on an unstable connection.\\n\\nLastly, there's the aspect of innovation and exploration. When you have full control over your setup, it can be easier to experiment and try out new ideas without being constrained by what a third-party service offers. You can iterate quickly, test different approaches, and refine your models more effectively.\\n\\nSo putting all these points together, running AI models locally seems beneficial in situations where data privacy is crucial, real-time processing is needed, specific model requirements exist, cost efficiency is important, integration with existing systems is necessary, customization is required, scalability is needed, connectivity issues arise, or innovation and experimentation are priorities. It might not always be better than cloud services, but there are definitely scenarios where local execution offers advantages.\\n</think>\\n\\nRunning AI models locally on a personal machine can offer several advantages depending on the specific needs and context:\\n\\n1. **Data Privacy and Control**: Handling sensitive data locally ensures that it isn't exposed to third-party servers, which is crucial for industries with strict privacy requirements.\\n\\n2. **Latency and Speed**: Real-time applications benefit from local processing, reducing delays caused by remote server responses.\\n\\n3. **Model Flexibility and Customization**: Accessing specific models or experimenting with configurations not available on cloud platforms allows for deeper customization and fine-tuning.\\n\\n4. **Cost Efficiency**: For large-scale operations, the potential cost savings from avoiding cloud service fees may outweigh initial setup costs.\\n\\n5. **Integration and Interoperability**: Local setups can more easily integrate with existing systems, enhancing overall efficiency and compatibility.\\n\\n6. **Customization and Research**: Local access facilitates deeper experimentation and analysis, beneficial for academic and developmental purposes.\\n\\n7. **Scalability**: Building a powerful local setup allows handling complex computations beyond typical cloud instances, accommodating large datasets.\\n\\n8. **Accessibility and Reliability**: In areas with poor internet connectivity, local processing ensures functionality without relying on unstable connections.\\n\\n9. **Innovation and Exploration**: Full control over the environment supports quick iteration and testing of new ideas, fostering innovation in AI development.\\n\\nIn summary, running AI models locally is advantageous for scenarios prioritizing data privacy, real-time needs, specific model requirements, cost efficiency, system integration, customization, scalability, reliability, and innovation. While cloud services offer benefits too, local execution can be more suitable in these specific contexts.\", 'chain2': '\\n\\n- **Privacy and Data Control**\\n- **Control and Flexibility**\\n- **Cost Savings**\\n- **Customization**\\n- **Speed and Efficiency**\\n- **Software Freedom**\\n- **Avoiding Vendor Lock-In**\\n- **Latency Reduction**\\n- **Technical Advantages**'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "detailedResponseChain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "headingInfoTemplate = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Analysis the resposne and get me just the heading from the {response}\n",
    "\n",
    "Response should be in bullet points \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chainWithHeading = { \"response\": detailedResponseChain } | headingInfoTemplate | llm2 | StrOutputParser()\n",
    "\n",
    "parallelRunnables = RunnableParallel(chain1=detailedResponseChain, chain2=chainWithHeading)\n",
    "\n",
    "response = parallelRunnables.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(response['chain1'])\n",
    "print('\\n\\n')\n",
    "print(response['chain2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ca265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "\n",
    "localMachineTemplate = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "localMachineChain = localMachineTemplate | llm | StrOutputParser()\n",
    "\n",
    "cloudMachineTemplate = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Analysis the resposne and get me just the heading from the {machine} \n",
    "\n",
    "Response should be in bullet points \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cloudMachineChain = cloudMachineTemplate | llm2 | StrOutputParser()\n",
    "\n",
    "parallelRunnables = RunnableParallel(chain1=localMachineChain, chain2=cloudMachineChain)\n",
    "\n",
    "response = parallelRunnables.invoke({\"env\": \"local machine\", \"machine\": \"cloud machine\"})\n",
    "\n",
    "print(response['chain1'])\n",
    "print('\\n\\n')\n",
    "print(response['chain2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26711768",
   "metadata": {},
   "source": [
    "### 运行 Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "detailedResponseChain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "headingInfoTemplate = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Analysis the resposne and get me just the heading from the {response}\n",
    "\n",
    "Response should be in bullet points \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def choose_llm(response):\n",
    "    response_text = str(response)\n",
    "    if len(response_text) < 300:\n",
    "        return llm2\n",
    "    return llm\n",
    "\n",
    "llm_selector = RunnableLambda(choose_llm)\n",
    "\n",
    "chainWithHeading = { \"response\": detailedResponseChain } | headingInfoTemplate | llm_selector | StrOutputParser()\n",
    "\n",
    "parallelRunnables = RunnableParallel(chain1=detailedResponseChain, chain2=chainWithHeading)\n",
    "\n",
    "response = parallelRunnables.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(response['chain1'])\n",
    "print('\\n\\n')\n",
    "print(response['chain2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72fdea",
   "metadata": {},
   "source": [
    "### 使用 @Chain 装饰器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an LLM expert\"),\n",
    "    (\"user\", \"What is the advantage of running AI Models in {env}\")\n",
    "])\n",
    "\n",
    "detailedResponseChain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "headingInfoTemplate = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Analysis the resposne and get me just the heading from the {response}\n",
    "\n",
    "Response should be in bullet points \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "@chain\n",
    "def choose_llm(response):\n",
    "    response_text = str(response)\n",
    "    if len(response_text) < 300:\n",
    "        return llm2\n",
    "    return llm\n",
    "\n",
    "llm_selector = RunnableLambda(choose_llm)\n",
    "\n",
    "chainWithHeading = { \"response\": detailedResponseChain } | headingInfoTemplate | llm_selector | StrOutputParser()\n",
    "\n",
    "parallelRunnables = RunnableParallel(chain1=detailedResponseChain, chain2=chainWithHeading)\n",
    "\n",
    "response = parallelRunnables.invoke({\"env\": \"local machine\"})\n",
    "\n",
    "print(response['chain1'])\n",
    "print('\\n\\n')\n",
    "print(response['chain2'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
